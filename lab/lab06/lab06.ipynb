{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f070db7-e5f4-42f8-9670-c5ba8b85fc96",
   "metadata": {},
   "source": [
    "# Lab06: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf146b-6d67-4d27-8ce9-5a623db9291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import types, importlib, requests, sys, re\n",
    "\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54547489-3660-4af2-8a53-62efd8ea1ff9",
   "metadata": {},
   "source": [
    "# Q1: Scrape Title\n",
    "\n",
    "**Objective:**\n",
    "Scrape all the titles of the books listed on the Books to Scrape website (http://books.toscrape.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8690080-b742-4da0-91d5-bd61c2168101",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Instructions:**\n",
    "- Use requests to send a GET request to the Books to Scrape homepage.\n",
    "- Parse the HTML content using BeautifulSoup.\n",
    "- Extract the book titles, which are inside \"h3\" tags with a nested \"a\" tag, and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf03a9-6c52-4537-b6da-e78be37204ff",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Send GET request to the website\n",
    "response = ...\n",
    "# Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(...)\n",
    "# Find all the title tags and print them\n",
    "titles = ...\n",
    "title_list = [title.text.strip() for title in titles]\n",
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7ba23",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e49e8-fb56-48d7-8a23-bdb67abba73e",
   "metadata": {},
   "source": [
    "# Q2: Web Scraping (headlines)\n",
    "**Objective:**\n",
    "Scrape headlines from the website https://news.ycombinator.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53b302-603b-4b72-b80d-7d727e37913c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:**\n",
    "- Send a GET request to the URL using requests.\n",
    "- Parse the HTML content with BeautifulSoup.\n",
    "- Extract and print the headlines inside <a> tags with class storylink.\n",
    "Handle cases where no headlines are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff935b-2526-4d20-bae8-d631c4e91223",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# URL of the news site to scrape\n",
    "url = \"https://news.ycombinator.com/\"\n",
    "\n",
    "# Send GET request to the website\n",
    "response = ...\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "# Parse the page content using BeautifulSoup\n",
    "soup = ...\n",
    "\n",
    "# Debugging: Print the raw HTML content to see what we are working with\n",
    "print(soup.prettify()[:1000])  # Print first 1000 characters to inspect the HTML\n",
    "\n",
    "# Find all the headline links (they are in <a> tags with class 'storylink')\n",
    "headlines = ...\n",
    "# Print each headline\n",
    "title_list = [title.text.strip() for title in headlines]\n",
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0bc2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b11943-6a48-44e0-8d8e-a27867065125",
   "metadata": {},
   "source": [
    "# Q3. Web Scraping (links)\n",
    "**Objective:**\n",
    "Scrape all the links (from <a> tags) from the Wikipedia homepage (https://www.wikipedia.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2bd708-58ef-48a1-810d-bee5f4af161a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Instructions:**\n",
    "- Use requests to send a GET request to the Wikipedia homepage.\n",
    "- Parse the HTML content using BeautifulSoup.\n",
    "- Extract all the links (href attributes) from <a> tags and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fab745-f9f6-4b36-b692-dd7399024b72",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"https://www.wikipedia.org/\"\n",
    "\n",
    "# Send GET request to the website\n",
    "response = ...\n",
    "\n",
    "# Parse the page content using BeautifulSoup\n",
    "soup = ...\n",
    "\n",
    "# Find all the anchor tags (<a>) which contain links\n",
    "links = ...\n",
    "# Print every non-empty href\n",
    "href_list = [link.get(\"href\") for link in links if link.get(\"href\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfc56a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7532bb-519d-4f64-839d-890553fb1ccd",
   "metadata": {},
   "source": [
    "# Q4. Web Scraping (quote texts)\n",
    "**Objective:**\n",
    "Scrape all the quote texts from the Quotes to Scrape website (https://quotes.toscrape.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f46a1b-abe4-4e23-b54d-a724ede95f19",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Instructions:**\n",
    "- Use requests to send a GET request to the Quotes to Scrape homepage.\n",
    "- Parse the HTML content using BeautifulSoup.\n",
    "- Extract all the quote texts, which are inside <span> tags with the class 'text', and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014631c-81a5-416a-a9a4-3374e61b5fb1",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# URL of the blog to scrape\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "\n",
    "# Send GET request to the website\n",
    "response = ...\n",
    "\n",
    "# Parse the page content using BeautifulSoup\n",
    "soup = ..\n",
    "\n",
    "# Find all the article titles (here, they are in <span> tags with class 'text')\n",
    "titles = ...\n",
    "# Print each quote\n",
    "title_list = [title.text.strip() for title in titles]\n",
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938fa3a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2a2b6-bde2-4767-9184-2f2b741a7a43",
   "metadata": {},
   "source": [
    "# Q5. Scraping a Simple Book Store Website for:\n",
    "    - Book titles\n",
    "    - Prices\n",
    "    - Ratings\n",
    "    - Availability\n",
    "- URL: http://books.toscrape.com/\n",
    "- Here, you will work with a scraper example (Book Titles + Prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab538c-58f0-4dda-9a27-26531536fc49",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"http://books.toscrape.com/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "soup = ...\n",
    "\n",
    "# Find all book containers\n",
    "books = ...\n",
    "\n",
    "# Extract title and price\n",
    "book_data = []\n",
    "for book in books:\n",
    "    title = ...\n",
    "    price = ...\n",
    "    book_data.append((title, price))\n",
    "\n",
    "# Show first 5\n",
    "for title, price in book_data[:5]:\n",
    "    print(f\"{title} — {price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e05aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38992b08-fbda-4114-8260-62eaf61a17e9",
   "metadata": {},
   "source": [
    "# Q6: Web Scraping eBay for Product Listings\n",
    "\n",
    "**Objective:** Write a Python script to scrape eBay search results for a specific product (e.g., laptops), extract product details, and display them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60696741-98af-4717-8ee6-109c9dc836cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Use the requests library to send an HTTP GET request to eBay’s search results page for a given product (e.g., \"laptop\").\n",
    "- Use the BeautifulSoup library to parse the HTML content of the response.\n",
    "- Extract the following details for each product listing:\n",
    "  - Title: The name or description of the product.\n",
    "  - Price: The price of the product.\n",
    "  - Hit Count: The number of views or watchers for the product.\n",
    "  - Display the extracted data in a readable format with product index, title, price, and hit count.\n",
    "  - Handle cases where some data may be missing by providing default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f6180-5664-4623-8865-ef3399ef4f7a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"https://www.ebay.com/sch/i.html?_nkw=laptop\"\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569a73f-e2de-40d2-bc3f-ea901cb69819",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Lab6! \n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7075b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90171eab",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b7ef0",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un5550fa25",
   "language": "python",
   "name": "un5550fa25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "lab06",
   "tests": {
    "q1": {
     "name": "q1",
     "points": [
      1,
      1,
      1,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 'Himalayas' in title_list[-1]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(title_list) == 20\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((isinstance(t, str) for t in title_list))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sum(('...' in t for t in title_list)) >= 5\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": [
      3,
      3,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> title_list[-1] == 'More'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(title_list) == 61\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> title_list[0] == '1.'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": [
      3,
      3,
      3,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(href_list) == 371\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> expected_slice = ['//ja.wikipedia.org/', '//ru.wikipedia.org/', '//de.wikipedia.org/', '//es.wikipedia.org/']\n>>> href_list[1:5] == expected_slice\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(href_list) > 0\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((isinstance(href, str) and href.strip() != '' for href in href_list))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": [
      3,
      3,
      3
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(title_list) == 10\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((isinstance(t, str) and t.strip() != '' for t in title_list))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((quote.startswith('“') and quote.endswith('”') for quote in title_list))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": [
      2,
      2,
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(book_data) == 20\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(book_data, list)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all((isinstance(item, tuple) and len(item) == 2 for item in book_data))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(('<' not in title and '>' not in title for (title, _) in book_data))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
